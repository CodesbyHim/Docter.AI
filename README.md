# ğŸ©º Doctor.AI â€“ AI-Powered Virtual Medical Assistant

**Doctor.AI** is an AI-powered virtual medical assistant that enables voice + text interactions between patients and doctors.
It leverages Speech Recognition, Large Language Models (LLMs), and Text-to-Speech to simulate realistic doctor-patient conversations, with support for multimodal queries (text + image).

âš ï¸ **Disclaimer:** This project is for educational and research purposes only. It is not a substitute for professional medical advice, diagnosis, or treatment.

# ğŸš€ Features

- ğŸ™ï¸ **Voice Input & Output:** Patients can speak naturally, and the doctor replies back with synthesized voice.
- ğŸ¤– **AI-Powered Responses:** Uses Groq-hosted LLaMA models for generating medical advice.
- ğŸ–¼ï¸ **Multimodal Support:** Accepts images + text queries (e.g., skin issues, scans).
- ğŸ”Š **Text-to-Speech Conversion:**
  - gTTS â€“ Lightweight, supports English + Hindi
  - ElevenLabs API â€“ Realistic AI voices (eleven_turbo_v2)
- ğŸ’¬ **Interactive UI:** Built with Gradio for seamless interaction.
- ğŸ“‚ **Lightweight Setup:** Only requires dependencies from requirements.txt.


#  ğŸ“‚ Project Structure
```
Doctor.AI/
â”‚
â”œâ”€â”€ app.py                    # Main entry point for running the app, Gradio UI for interactions with language choice 
â”œâ”€â”€ gradio_app.py             # another Gradio UI for interactions  
â”œâ”€â”€ brain_of_the_doctor.py    # Core AI logic (LLM + image analysis)  
â”œâ”€â”€ voice_of_the_doctor.py    # Text-to-Speech (gTTS + ElevenLabs)  
â”œâ”€â”€ voice_of_the_patient.py   # Speech-to-Text (Groq Whisper API)  
â”œâ”€â”€ requirements.txt          # Python dependencies  
â”œâ”€â”€ presentation.pdf          # Project presentation slides  
â””â”€â”€ README.md                 # Project documentation
```

# âš™ï¸ Technical Details

- Speech-to-Text (STT):
  - Model: Whisper-large-v3
  - API: Groq API

- Text-to-Speech (TTS):
  - gTTS (Google TTS) â†’ Lightweight, bilingual (EN/HI)
  - ElevenLabs API â†’ Natural AI voices (eleven_turbo_v2)

- Core AI Model (Doctorâ€™s Brain):
  - Model: Meta-LLaMA 4 Scout 17B Instruct
  - API: Groq API

- Multimodal Support:
  - Image queries are base64-encoded and analyzed alongside text prompts

- Frameworks/Libraries Used:
  - Gradio â€“ Web UI
  - speech_recognition + pydub â€“ Audio recording & processing
  - gtts, elevenlabs â€“ TTS engines
  - langdetect â€“ Language detection
  - dotenv â€“ Environment management

- Environment Variables (from .env):
```
GROQ_API_KEY=your_groq_api_key_here
ELEVEN_API_KEY=your_elevenlabs_api_key_here
```

# âš™ï¸ Installation

Clone the Repository
```
git clone https://github.com/your-username/Doctor.AI.git
cd Doctor.AI
```

Create Virtual Environment
```
python -m venv venv
source venv/bin/activate   # On Linux/Mac
venv\Scripts\activate      # On Windows
```

Install Dependencies
```
pip install -r requirements.txt
```

â–¶ï¸ Usage
Run the app locally:
```
python app.py
```

The app will start a Gradio server at:
ğŸ‘‰ http://127.0.0.1:7860

To create a public share link, modify launch() in gradio_app.py:
```
interface.launch(share=True)
```

# ğŸ“– Workflow

- ğŸ—£ï¸ Patient speaks â†’ converted into text via Groq Whisper-large-v3
- ğŸ§  AI model (LLaMA) processes query + optional image â†’ generates doctorâ€™s response
- ğŸ©º Doctorâ€™s response â†’ converted into speech using gTTS or ElevenLabs API
- ğŸ§ Output displayed in Gradio UI as text + audio playback

# ğŸ“Š Presentation

ğŸ“‘ See [Project Presentation](./presentation.pdf)
 for a detailed project walkthrough.

# ğŸ¤ Contributing

Contributions are welcome! ğŸ‰
Fork the repo, create a new branch, and submit a pull request.

# ğŸ“œ License

This project is licensed under the MIT License.
